{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG, TD3, SAC, A2C, PPO for continuous actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from stable_baselines3 import SAC, DDPG, TD3,PPO,A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "env = gym.make(\"LunarLanderContinuous\", render_mode=\"\")\n",
    "\"\"\"\n",
    "{'Swimmer', 'Taxi', 'InvertedPendulum',\n",
    " 'Walker2d', 'Ant', 'Blackjack',\n",
    "  'Pendulum', 'LunarLanderContinuous',\n",
    "   'Pusher', 'CliffWalking', 'Acrobot',\n",
    "    'MountainCar', 'BipedalWalker',\n",
    "     'GymV21Environment', 'LunarLander',\n",
    "      'InvertedDoublePendulum', 'FrozenLake8x8',\n",
    "       'HumanoidStandup', 'Hopper', 'FrozenLake',\n",
    "        'CarRacing', 'HalfCheetah', 'BipedalWalkerHardcore',\n",
    "        'CartPole', 'MountainCarContinuous', 'Humanoid',\n",
    "         'GymV26Environment', 'Reacher'}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    #buffer_size=int(1e3),\n",
    "    #learning_rate=1e-3,\n",
    "    #gamma=0.95,\n",
    "    #batch_size=256,\n",
    "    tensorboard_log=\"./output/DRL/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TD3(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"./output/DRL/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPG(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"./output/DRL/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"./output/DRL/tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PPO(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"./PPO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env.reset()\n",
    "\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(\n",
    "    total_timesteps=int(2e6),#2e5\n",
    "    progress_bar=True,\n",
    "    log_interval=1,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN ALL IN PARALLEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from stable_baselines3 import SAC, DDPG, TD3,PPO,A2C\n",
    "\n",
    "def get_env():\n",
    "    return gym.make(\"LunarLanderContinuous\", render_mode=\"\")\n",
    "\n",
    "def do(model):\n",
    "    model.learn(\n",
    "        total_timesteps=int(1e6),\n",
    "        progress_bar=False,\n",
    "        log_interval=1,\n",
    "    )\n",
    "    print(\"done：\",model)\n",
    "\n",
    "agents ={\n",
    "    \"SAC\":SAC(\"MlpPolicy\", get_env(), verbose=0,tensorboard_log=\"./output/DRL/tensorboard/\",stats_window_size=10),\n",
    "    \"TD3\":TD3(\"MlpPolicy\", get_env(), verbose=0,tensorboard_log=\"./output/DRL/tensorboard/\",stats_window_size=10),\n",
    "    \"DDPG\":DDPG(\"MlpPolicy\", get_env(), verbose=0,tensorboard_log=\"./output/DRL/tensorboard/\",stats_window_size=10),\n",
    "    \"A2C\":A2C(\"MlpPolicy\", get_env(), verbose=0,tensorboard_log=\"./output/DRL/tensorboard/\",stats_window_size=10),\n",
    "    \"PPO\":PPO(\"MlpPolicy\", get_env(), verbose=0,tensorboard_log=\"./output/DRL/tensorboard/\",stats_window_size=10),\n",
    "}\n",
    "\n",
    "for name,agent in agents.items():\n",
    "    print(name)\n",
    "    t = Thread(target=do, args=(agent,))\n",
    "    t.start()\n",
    "\n",
    "print(\"all done！\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIGURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def average_filter(data, window_size):\n",
    "    \"\"\"平均滤波函数\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): 输入的数据，shape为(n,)\n",
    "        window_size (int): 窗口大小\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 平滑后的数据，shape为(n,)\n",
    "    \"\"\"\n",
    "    result = np.zeros_like(data)\n",
    "    for i in range(window_size // 2, len(data) - window_size // 2):\n",
    "        result[i] = np.mean(data[i - window_size // 2: i + window_size // 2])\n",
    "    return result\n",
    "\n",
    "def expo_filter(data, alpha):\n",
    "    \"\"\"指数滤波函数\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): 输入的数据，shape为(n,)\n",
    "        alpha (float): 平滑因子，取值在(0, 1]之间\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: 平滑后的数据，shape为(n,)\n",
    "    \"\"\"\n",
    "    result = np.zeros_like(data)\n",
    "    result[0] = data[0]\n",
    "    for i in range(1, len(data)):\n",
    "        result[i] = alpha * data[i] + (1 - alpha) * result[i - 1]\n",
    "    return result\n",
    "\n",
    "def find_nearest(array,v):\n",
    "    \"\"\"找到array中距离v最近的值的索引\n",
    "    \n",
    "    Args:\n",
    "        array (np.ndarray): 输入的数据，shape为(n,)\n",
    "        v (float): 目标值\n",
    "        \n",
    "    Returns:\n",
    "        int: 最近的值的索引\n",
    "    \"\"\"\n",
    "    return np.argmin(np.abs(array - v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def load(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        agent_reward = json.load(f)\n",
    "    timestamp = [t[1] for t in agent_reward]\n",
    "    reward = [t[2] for t in agent_reward]\n",
    "    return np.array(timestamp), np.array(reward)\n",
    "\n",
    "#遍历文件夹中的文件\n",
    "def walkFile(file,filter=False):\n",
    "    for root, dirs, files in os.walk(file):\n",
    "        for f in files:\n",
    "            if f.endswith(\".json\"):\n",
    "                filepath = os.path.join(root, f)\n",
    "                print(filepath)\n",
    "                timestamp, reward = load(filepath)\n",
    "                if filter:\n",
    "                    # 平均滤波\n",
    "                    reward = average_filter(reward, 5)\n",
    "                    # 指数滤波\n",
    "                    #reward = expo_filter(reward, 0.1)\n",
    "\n",
    "                plot(timestamp, reward, label=f.split(\".\")[0])\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "def plot(timestamp, reward, label):\n",
    "    color = next(colors)\n",
    "    plt.plot(timestamp, reward,c=color, label=label)\n",
    "    draw_shadow(timestamp,reward,color)\n",
    "\n",
    "\n",
    "def draw_shadow(x, y,c,window_size=20):\n",
    "\n",
    "    # Iterate through the data with a sliding window\n",
    "    for i in range(0,len(x) - window_size + 1,window_size):\n",
    "        data = y[i:i + window_size]\n",
    "        #mean = np.mean(data)\n",
    "        std = np.std(data)*5\n",
    "        # Plot the shadow\n",
    "        plt.fill_between(x[i:i + window_size], data - std/2, data + std/2, alpha=0.2, color=c)\n",
    "        \n",
    "# Create a color cycle iterator\n",
    "colors = iter(plt.cm.Set1(np.linspace(0, 1, 5))) \n",
    "plt.figure(figsize=(12, 4), dpi=300)\n",
    "walkFile(\"./output/DRL/10/avg_reward\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "#Y轴范围\n",
    "plt.ylim(-1000, 500)\n",
    "#X轴范围\n",
    "plt.xlim(0, 1e6)\n",
    "plt.savefig(\"./output/DRL/avg_reward.png\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a color cycle iterator\n",
    "colors = iter(plt.cm.Set1(np.linspace(0, 1, 5))) \n",
    "plt.figure(figsize=(12, 4), dpi=300)\n",
    "walkFile(\"./output/DRL/10/survival_episode\",filter=True)\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Average Survival Episodes\")\n",
    "plt.ticklabel_format(style='plain', axis='x')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "#Y轴范围\n",
    "plt.ylim(-400, 1200)\n",
    "#X轴范围\n",
    "plt.xlim(0, 1e6)\n",
    "plt.savefig(\"./output/DRL/survival_episode.png\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "308fc2fe6b067f030c5852dab12bb33848fd6a9fa5aaaacd572d50d700aa3651"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
